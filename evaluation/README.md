# Evaluation Tools Directory

This directory contains **tools and infrastructure** for SeizureTransformer evaluation.

**âš ï¸ Note**: Experimental **results** are stored in `../experiments/` directory.

## Directory Structure

```
evaluation/
â”œâ”€â”€ tusz/                   # TUSZ dataset evaluation scripts
â”‚   â””â”€â”€ run_tusz_eval.py    # Main evaluation script (generates checkpoints)
â”œâ”€â”€ nedc_scoring/           # NEDC pipeline tools
â”‚   â”œâ”€â”€ convert_predictions.py
â”‚   â”œâ”€â”€ run_nedc.py
â”‚   â”œâ”€â”€ sweep_operating_point.py
â”‚   â””â”€â”€ post_processing.py
â””â”€â”€ nedc_eeg_eval/v6.0.0/   # Official NEDC binaries and libraries
```

## Tools vs Results Separation

### ğŸ› ï¸ Tools (Stay in `evaluation/`)
- **Scripts**: Evaluation pipelines, converters, scorers
- **Binaries**: NEDC v6.0.0 official evaluation software
- **Infrastructure**: Reusable components across experiments

### ğŸ“Š Results (Go to `experiments/`)
- **Checkpoints**: Model predictions (*.pkl files)
- **Metrics**: NEDC scoring outputs, JSON summaries
- **Logs**: Execution logs, parameter records

## Running Evaluations

### Generate Predictions
```bash
# Create experiment checkpoint
python evaluation/tusz/run_tusz_eval.py \
  --data_dir /path/to/TUSZ/eval \
  --out_dir experiments/eval/my_experiment \
  --device auto
```

### Score with NEDC
```bash
# Run NEDC pipeline
cd evaluation/nedc_scoring
make all CHECKPOINT=../../experiments/eval/my_experiment/checkpoint.pkl
```

### Sweep Operating Points
```bash
# Parameter optimization (dev split only)
python evaluation/nedc_scoring/sweep_operating_point.py \
  --checkpoint experiments/dev/baseline/checkpoint.pkl \
  --target_fa_per_24h 10
```

## Migration Complete

âœ… **Moved to experiments/eval/baseline/**:
- `checkpoint.pkl` (470MB baseline predictions)
- `results.json` (AUROC and sample-level metrics)  
- `eval_log.txt` (execution log)

âœ… **Remaining in evaluation/tusz/**:
- `run_tusz_eval.py` (evaluation script - reusable tool)

This organization follows ML best practices:
- **Tools are version-controlled and shared**
- **Results are experiment-specific and tracked separately**
- **Clear separation between infrastructure and outputs**
