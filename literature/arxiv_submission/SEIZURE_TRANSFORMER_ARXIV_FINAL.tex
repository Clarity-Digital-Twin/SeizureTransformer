\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

The evaluation of seizure detection algorithms suffers from a
fundamental reproducibility crisis. Published models report dramatically
different performance metrics while claiming state-of-the-art results,
yet these claims often cannot be verified due to proprietary datasets,
missing evaluation code, or ambiguous scoring definitions. This opacity
particularly affects deep learning approaches, where complex
architectures and extensive hyperparameter spaces compound the
verification challenge.

SeizureTransformer {[}1{]} exemplifies this problem. The model reports
80.30\% sensitivity at 1.0 FA/24h on a proprietary Danish dataset
(Dianalund), with no public evaluation possible. While the authors
released model weights---a commendable step toward reproducibility---the
absence of standardized evaluation on public datasets leaves critical
questions unanswered about real-world performance.

We present, to our knowledge, the first evaluation of SeizureTransformer
on TUSZ's held-out test set using Temple's NEDC v6.0.0 scoring software.
Our systematic comparison evaluates identical model predictions using
three scoring methodologies: NEDC TAES (time-aligned event scoring),
NEDC OVERLAP (binary any-overlap), and SzCORE Event. At the paper's
default parameters (threshold=0.8, kernel=5, duration=2.0s), we observe
45.63\% sensitivity at 26.89 FA/24h with NEDC OVERLAP---a 27-fold
increase from the Dianalund benchmark claim. The same predictions yield
136.73 FA/24h with NEDC TAES (137-fold increase) and 8.59 FA/24h with
SzCORE Event. This 3.1-fold difference between NEDC OVERLAP and SzCORE
Event stems entirely from scoring methodology, independent of model
architecture or parameters.

\begin{figure}
\hypertarget{fig:performance-gap}{%
\centering
\includegraphics[width=1\textwidth,height=\textheight]{figures/output/arxiv/fig1_performance_gap.pdf}
\caption{Performance gap visualization showing the 27--137x difference
between claimed and measured false alarm rates. Panel A shows false
alarm rates on a logarithmic scale, comparing Dianalund's claimed
performance (1 FA/24h) against our TUSZ evaluation using different
scoring methods. Panel B displays sensitivity near 10 FA/24h using each
scorer's closest available operating point (no interpolation). SzCORE
Event uses any-overlap with clinical tolerances (-30 s/+60 s; merge
\textless90 s, split \textgreater5 min).}\label{fig:performance-gap}
}
\end{figure}

Our contributions extend beyond revealing performance gaps. We provide:
(1) a reproducible NEDC v6.0.0 evaluation pipeline for TUSZ, bridging
the research-to-clinic evaluation gap {[}2{]}; (2) comprehensive
operating points for clinical deployment, including evaluation at a
clinically-motivated threshold of \textless=10 FA/24h; (3) quantitative
evidence that scoring methodology alone can account for multi-fold
performance differences, highlighting the critical need for transparent
reporting; and (4) open-source infrastructure enabling the community to
replicate and extend our evaluation framework. When optimizing for the
10 FA/24h threshold, SeizureTransformer achieves only 33.90\%
sensitivity with NEDC OVERLAP, falling far short of the 75\% sensitivity
goal for clinical systems {[}3{]}. The remainder of this paper is
organized as follows. Section 2 provides background and related work on
TUSZ, NEDC, and scoring methodologies. Section 3 details our evaluation
methodology, including data preparation, model inference, and
multi-scorer validation. Section 4 presents comprehensive results across
multiple operating points and scoring methods. Section 5 discusses
implications for clinical deployment, the need for standardized
evaluation, and limitations of current benchmarking practices. Section 6
concludes. Section 7 outlines reproducibility resources and exact rerun
procedures.

\hypertarget{background-and-related-work}{%
\section{Background and Related
Work}\label{background-and-related-work}}

The Temple University Hospital Seizure Corpus (TUSZ) is the largest
publicly available seizure dataset {[}4{]}. Critically, TUSZ implements
patient-disjoint train/dev/eval splits---no patient appears in multiple
splits---preventing data leakage and enabling valid generalization
assessment. The evaluation set contains 865 EDF files totaling 127.7
hours from 43 patients with 469 seizures, specifically reserved for
final held-out testing {[}4{]}. This careful split design follows
machine learning best practices often violated in medical AI
applications. The annotations, performed by board-certified neurologists
at Temple University Hospital, follow conservative clinical guidelines
requiring clear electrographic seizures with definite evolution and
temporal precision in marking onset and offset.

Alongside TUSZ, Temple University developed the Neural Event Detection
Competition (NEDC) scoring software suite, creating a matched evaluation
ecosystem {[}2{]}. NEDC v6.0.0 provides the definitive scoring
implementation for TUSZ evaluation {[}2{]}. This matched pairing is no
coincidence---the same research group created both the dataset and its
evaluation tools, ensuring consistency between annotation philosophy and
scoring methodology. NEDC implements multiple scoring modes, with
OVERLAP (any-overlap binary scoring) serving as the commonly reported
evaluation mode for TUSZ. The software is widely used in the literature
and serves as a reference implementation for seizure detection
evaluation {[}5, 2{]}.

The choice of scoring methodology profoundly impacts reported
performance, as different methods serve distinct clinical and research
priorities. Time-Aligned Event Scoring (TAES), proposed by Shah et
al.~{[}5{]}, represents the strictest evaluation standard, computing
partial credit based on temporal overlap percentage---a 60-second
seizure with 45 seconds correctly detected receives 0.75 true positive
credit {[}5{]}. TAES emphasizes temporal precision, making it ideal for
algorithm development and research applications where exact timing
matters. In contrast, OVERLAP scoring, which NEDC implements as a
primary mode, treats any temporal overlap between prediction and ground
truth as a full true positive {[}2{]}. Shah et al.~{[}5{]} note that
``OVLP is considered a very permissive way of scoring since any amount
of overlap between a reference and hypothesis event constitutes a true
positive,'' yet this binary approach has become a de facto standard for
TUSZ reporting, balancing clinical relevance with research needs.

At the most permissive end of the spectrum, SzCORE Event {[}6{]} extends
any-overlap scoring with clinical tolerances designed for real-world
deployment. The system adds 30-second pre-ictal and 60-second post-ictal
windows around each ground truth event {[}6{]}, recognizing that early
warnings before seizure onset provide clinical value and that EEG
patterns normalize gradually after seizure termination. Additionally,
SzCORE Event merges predictions separated by less than 90 seconds into
single events, substantially reducing alarm fatigue in clinical settings
{[}6{]}. These modifications, while clinically motivated, can reduce
reported false alarm rates by factors of 3-10x compared to stricter
scoring methods {[}6{]}. Importantly, these different approaches
represent not right or wrong methods but rather different valid
perspectives on what constitutes meaningful seizure detection---research
precision versus clinical utility versus deployment practicality.

Scoring paradigms broadly fall into event-based (e.g., OVERLAP, TAES,
SzCORE Event) and sample-based (epoch) methods. Event-based evaluation
aligns with clinical interpretation of seizures as discrete neurological
events. Sample-based methods compare 1 Hz labels and can conflate
non-seizure accuracy with seizure detection utility; therefore, all
results in this work use event-based metrics. See Methods for precise
definitions and the rationale for this choice.

SeizureTransformer {[}1{]} exemplifies both the advances and evaluation
gaps in modern seizure detection. The architecture combines U-Net's
biomedical segmentation capabilities with Transformer self-attention to
capture local and global EEG patterns {[}1{]}. Trained on a subset of
TUSZ v1.5.2 (\textasciitilde910 hours) {[}4{]} plus the Siena Scalp EEG
Database (128 hours) {[}7{]}, the model processes 19-channel EEG at 256
Hz through 60-second windows {[}1{]}. With roughly 41 million parameters
and publicly available pretrained weights (\textasciitilde=168 MB)
{[}1{]}, SeizureTransformer won the EpilepsyBench Challenge, achieving
37\% sensitivity at 1 false alarm per 24 hours on the Dianalund dataset
{[}8{]}---a Danish long-term monitoring corpus distinct from its
training data. The authors' decision to openly share their weights
enables reproducible evaluation, a practice we build on here {[}1{]}.

Despite training on TUSZ, SeizureTransformer has never been evaluated on
TUSZ's held-out evaluation set using Temple's official scoring software
{[}2{]}. EpilepsyBench marks TUSZ results with a train emoji
({[}train{]}) {[}8{]}, indicating the model was trained on this dataset
and therefore showing no evaluation metrics. While this conservative
approach prevents overfitting claims, it overlooks the careful
patient-disjoint split design that specifically enables valid held-out
evaluation {[}4{]}. This represents a broader pattern in the field:
models are trained on Dataset X, evaluated on Dataset Y with favorable
scoring, generalization is claimed, yet performance on X's properly
designed evaluation set remains unknown {[}3, 9{]}. The uniform
application of SzCORE Event scoring across all EpilepsyBench datasets,
while ensuring consistency, obscures dataset-specific performance that
would be revealed by matched evaluation tools {[}8{]}.

The clinical deployment of seizure detection systems requires meeting
stringent performance thresholds. Clinical goals typically target 75\%
sensitivity or higher {[}3{]}, while human reviewers achieve
approximately 1 false alarm per 24 hours {[}3{]}. These requirements
reflect the reality of clinical workflows where excessive false alarms
lead to alarm fatigue and system abandonment. However, whether a system
meets these thresholds depends critically on the evaluation methodology
employed. Previous work has highlighted challenges in cross-dataset
generalization {[}10{]}, the need for standardized evaluation metrics
{[}3{]}, and broader reproducibility issues in medical AI {[}9{]}. Our
work addresses these challenges by performing the missing evaluation:
testing SeizureTransformer on TUSZ's held-out set using multiple scoring
methodologies, revealing how evaluation choices fundamentally shape
performance claims in seizure detection systems.

\hypertarget{methods}{%
\section{Methods}\label{methods}}

We evaluated SeizureTransformer on the TUSZ v2.0.3 held-out test set
using the authors' pretrained weights without modification {[}1{]}. Our
evaluation employed three distinct scoring methodologies on identical
model predictions to quantify the impact of evaluation standards on
reported performance.

\hypertarget{dataset}{%
\subsection{Dataset}\label{dataset}}

We used the Temple University Hospital Seizure Corpus (TUSZ) v2.0.3,
focusing on its carefully designed evaluation split {[}4{]}. The eval
set contains 865 EDF files totaling 127.7 hours from 43 patients with
469 expert-annotated seizures {[}4{]}. Critically, this set is
patient-disjoint from the training and development splits, ensuring no
data leakage and enabling valid generalization assessment {[}4{]}. We
achieved 100\% file coverage, with one file requiring automated header
repair using pyEDFlib's repair functionality on a temporary copy
{[}11{]}.

The development set, containing 1,832 files (435.5 hours) from 53
distinct patients with 1,075 seizures, was used exclusively for
post-processing parameter optimization. This maintains the integrity of
the held-out evaluation while allowing systematic exploration of
clinical operating points.

\hypertarget{model-and-inference-pipeline}{%
\subsection{Model and Inference
Pipeline}\label{model-and-inference-pipeline}}

We employed the authors' publicly available pretrained
SeizureTransformer weights (\textasciitilde=168 MB) without any
modifications, retraining, or fine-tuning {[}1{]}. The model expects
19-channel unipolar montage EEG data sampled at 256 Hz, processing
60-second windows (15,360 samples per channel) through its
U-Net-Transformer architecture {[}1{]}.

Our preprocessing pipeline, implemented as a wrapper around the original
wu\_2025 code, largely follows the paper's specifications {[}1{]}. For
each EDF file, we: (1) load the data with unipolar montage enforcement
and normalized channel aliases; (2) apply per-channel z-score
normalization across the full recording; (3) resample to 256 Hz if
necessary; (4) apply a 0.5-120 Hz bandpass filter (3rd-order
Butterworth); and (5) apply notch filters at 1 Hz and 60 Hz (Q=30). The
1 Hz notch (to suppress heart-rate artifacts) reflects our released
evaluation code and is an addition beyond the paper's brief
preprocessing description {[}1{]}.

The model processes 60-second non-overlapping windows, outputting
per-sample seizure probabilities at 256 Hz. Post-processing applies
three sequential operations using configurable parameters: (1) threshold
the probability values to create a binary mask; (2) apply morphological
opening and closing operations with a specified kernel size; and (3)
remove events shorter than a minimum duration. The paper's default
configuration uses threshold theta=0.8, kernel size k=5 samples, and
minimum duration d=2.0 seconds {[}1{]}.

\hypertarget{scoring-methodologies}{%
\subsection{Scoring Methodologies}\label{scoring-methodologies}}

We evaluated identical model predictions using three scoring
methodologies, each representing different clinical and research
priorities:

\textbf{NEDC TAES (Time-Aligned Event Scoring)} computes partial credit
based on temporal overlap between predictions and ground truth {[}5{]}.
If a 60-second reference seizure has 45 seconds correctly detected, TAES
awards 0.75 true positive credit {[}5{]}. This methodology emphasizes
temporal precision, making it the strictest evaluation standard.

\textbf{NEDC OVERLAP} implements Temple's binary any-overlap scoring
within the NEDC v6.0.0 framework {[}2{]}. Any temporal overlap between
prediction and reference, regardless of duration, counts as a full true
positive. This represents the commonly reported mode for TUSZ
evaluation, matching the dataset's annotation philosophy {[}2{]}.

\textbf{SzCORE Event (Any-Overlap + tolerances)} extends binary scoring
with clinical tolerances: 30-second pre-ictal and 60-second post-ictal
windows around each reference event, plus merging of predictions
separated by less than 90 seconds {[}6{]}. These modifications, designed
for clinical deployment scenarios where early warnings and reduced alarm
fatigue are prioritized, substantially reduce reported false alarm rates
{[}6{]}.

All scoring implementations process the same binary prediction masks,
ensuring that performance differences stem solely from scoring
philosophy rather than model behavior.

\begin{figure}
\hypertarget{fig:operating-curves}{%
\centering
\includegraphics[width=1\textwidth,height=\textheight]{figures/output/arxiv/fig2_operating_curves.pdf}
\caption{Operating characteristic curves across scoring methodologies.
The same model predictions yield dramatically different
sensitivity-false alarm tradeoffs depending on scoring choice. The
clinical target zone (green) represents the desired operating region for
deployment (\textgreater=75\% sensitivity, \textless=10 FA/24h). The
paper's default operating point (black circle) falls far outside
clinical viability for all scoring methods on
TUSZ.}\label{fig:operating-curves}
}
\end{figure}

\hypertarget{choice-of-event-based-metrics}{%
\subsubsection{Choice of Event-Based
Metrics}\label{choice-of-event-based-metrics}}

We report only event-based metrics (NEDC OVERLAP, NEDC TAES, SzCORE
Event) because clinical evaluation focuses on detecting discrete seizure
events. Sample-based (epoch) methods (e.g., NEDC EPOCH; SzCORE
Sample-based) compare 1 Hz labels and can inflate scores by rewarding
long non-seizure periods, obscuring event detection quality. To avoid
this pitfall, all reported results are event-based. ``SzCORE Event''
denotes any-overlap with +-30 s/60 s tolerances and merge/split rules
(merge \textless90 s, split \textgreater5 min); we do not report SzCORE
Sample-based.

\hypertarget{parameter-optimization}{%
\subsection{Parameter Optimization}\label{parameter-optimization}}

We conducted systematic post-processing parameter optimization on the
TUSZ development set, targeting clinical deployment criteria of
\textless=10 false alarms per 24 hours while maximizing sensitivity. Our
grid search explored: thresholds theta in \{0.60, 0.65, 0.70, 0.75,
0.80, 0.85, 0.88, 0.90, 0.92, 0.95, 0.98\}, morphological kernel sizes k
in \{3, 5, 7, 9, 11, 13, 15\} samples, and minimum event durations d in
\{1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 6.0\} seconds.

For each configuration, we computed sensitivity and false alarm rates
using NEDC OVERLAP scoring, as this is the commonly reported mode for
TUSZ. From the resulting parameter space, we selected operating points
for comprehensive evaluation: (1) \textbf{Default} (theta=0.80, k=5,
d=2.0s) --- the paper's published configuration; (2) \textbf{Clinical 10
FA/24h target} (theta=0.88, k=5, d=3.0s) --- optimized to meet the
\textless=10 FA/24h constraint; and (3) \textbf{ICU-like 2.5 FA/24h
target} (theta=0.95, k=5, d=5.0s) --- a more conservative operating
point. We additionally report selected high-threshold points (e.g.,
theta=0.98) when illustrating the full trade-off curve.

\begin{figure}
\hypertarget{fig:scoring-impact}{%
\centering
\includegraphics[width=1\textwidth,height=\textheight]{figures/output/arxiv/fig3_scoring_impact.pdf}
\caption{Impact of scoring methodology on reported performance. The same
SeizureTransformer predictions flow through different scoring pipelines,
yielding a 15.9x difference in false alarm rates between NEDC TAES and
SzCORE Event. This visualization demonstrates how evaluation standards,
not model improvements, can account for order-of-magnitude performance
variations.}\label{fig:scoring-impact}
}
\end{figure}

\hypertarget{implementation-and-validation}{%
\subsection{Implementation and
Validation}\label{implementation-and-validation}}

Our evaluation pipeline integrates multiple software components to
ensure reproducibility and clinical validity. Model inference uses the
original wu\_2025 codebase with our preprocessing wrapper. Predictions
are converted to NEDC's CSV\_bi format, which requires specific
formatting: four decimal places for timestamps, ``TERM'' as the channel
identifier, and standardized header metadata including file duration.

We validated our implementation through consistency checks across tools
and confirmed that all 865 eval files were successfully processed (with
one automated header repair). Reported results use the official NEDC
scorers and SzCORE Event.

To enable full reproducibility, we provide our complete evaluation
codebase, including the preprocessing wrapper, scoring implementations,
and parameter optimization scripts. The pretrained SeizureTransformer
weights remain available from the authors' repository, and NEDC v6.0.0
can be obtained from Temple University.

\hypertarget{statistical-analysis}{%
\subsection{Statistical Analysis}\label{statistical-analysis}}

We report standard seizure detection metrics for each configuration and
scorer combination: sensitivity (seizure-level recall), false alarm rate
per 24 hours (computed from total recording duration), and F1 score. For
NEDC scorers, we report SEIZ-only FA/24h as the primary metric (Temple's
``Total FA'' is archived in summaries). For SzCORE Event, we follow its
event-based false positive definition. We also computed AUROC across
threshold values to assess overall discriminative capability independent
of operating point selection.

This comprehensive evaluation framework, combining the authors'
pretrained model with multiple scoring standards applied to a properly
held-out test set, reveals how methodological choices fundamentally
shape reported performance metrics in seizure detection systems.

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{evaluation-setup}{%
\subsection{Evaluation Setup}\label{evaluation-setup}}

We evaluated SeizureTransformer on TUSZ v2.0.3's held-out evaluation set
containing 865 EEG files (127.7 hours of recordings). Using the authors'
pretrained weights, we generated predictions and evaluated them using
three scoring methodologies: NEDC OVERLAP (Temple's official any-overlap
mode), NEDC TAES (time-aligned), and SzCORE Event (EpilepsyBench
standard).

\hypertarget{primary-results}{%
\subsection{Primary Results}\label{primary-results}}

\hypertarget{default-configuration-theta0.80-k5-d2.0}{%
\subsubsection{Default Configuration (theta=0.80, k=5,
d=2.0)}\label{default-configuration-theta0.80-k5-d2.0}}

At the paper's default parameters, we observed dramatic variation across
scoring methods. The same predictions yielded:

\begin{itemize}
\tightlist
\item
  \textbf{NEDC OVERLAP}: 45.63\% sensitivity, 26.89 FA/24h
\item
  \textbf{NEDC TAES}: 65.21\% sensitivity, 136.73 FA/24h
\item
  \textbf{SzCORE Event}: 52.35\% sensitivity, 8.59 FA/24h
\end{itemize}

This represents a \textbf{3.1x difference} in false alarm rates between
NEDC OVERLAP and SzCORE Event scoring on identical predictions. Compared
to the paper's reported \textasciitilde1 FA/24h on Dianalund, we observe
a \textbf{27-fold gap} with NEDC OVERLAP and a \textbf{137-fold gap}
with NEDC TAES.

\begin{longtable}[]{@{}lrrrr@{}}
\toprule
Scoring Method & Sensitivity (\%) & FA/24h & Multiplier vs Claimed & F1
Score\tabularnewline
\midrule
\endhead
\textbf{Dianalund (Claimed)} & 37.00 & 1.00 & 1x & 0.43*\tabularnewline
SzCORE Event & 52.35 & 8.59 & 9x & 0.485\tabularnewline
NEDC OVERLAP & 45.63 & 26.89 & \textbf{27x} & 0.396\tabularnewline
NEDC TAES & 65.21 & 136.73 & \textbf{137x} & 0.237\tabularnewline
\bottomrule
\end{longtable}

Table 1: Performance at default parameters (theta=0.80, k=5, d=2.0). *F1
from competition leaderboard.

\hypertarget{clinical-deployment-targets}{%
\subsubsection{Clinical Deployment
Targets}\label{clinical-deployment-targets}}

We optimized parameters on the development set to target clinical false
alarm thresholds:

\textbf{10 FA/24h Target (theta=0.88, k=5, d=3.0)}: - NEDC OVERLAP
achieved 33.90\% sensitivity at 10.27 FA/24h - While meeting our FA
constraint, this falls far below the 75\% sensitivity goal for clinical
systems {[}12{]} - SzCORE Event achieved 40.59\% sensitivity at only
3.36 FA/24h

\textbf{2.5 FA/24h Target (theta=0.95, k=5, d=5.0)}: - NEDC OVERLAP
achieved 14.50\% sensitivity at 2.05 FA/24h - Sensitivity too low for
clinical viability - SzCORE Event achieved 19.71\% sensitivity at 0.75
FA/24h

\begin{figure}
\hypertarget{fig:parameter-heatmap}{%
\centering
\includegraphics[width=1\textwidth,height=\textheight]{figures/output/arxiv/fig4_parameter_heatmap.pdf}
\caption{Parameter sensitivity analysis showing F1 scores across
threshold and minimum duration values for NEDC OVERLAP scoring. The
heatmaps reveal that optimal parameters vary by morphological kernel
size, with the paper's default (theta=0.8, d=2.0) marked. Higher
thresholds are required to achieve clinically acceptable false alarm
rates.}\label{fig:parameter-heatmap}
}
\end{figure}

\hypertarget{key-findings}{%
\subsection{Key Findings}\label{key-findings}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Scoring Impact}: The \textasciitilde=3.1x difference at
  default (NEDC OVERLAP vs SzCORE Event) stems entirely from scoring
  methodology, with TAES showing even larger divergence (5.1x vs
  OVERLAP).
\item
  \textbf{Clinical Viability}: SeizureTransformer cannot achieve
  clinical viability when evaluated with NEDC scoring on TUSZ. At 10
  FA/24h, it reaches only 33.90\% sensitivity, far below the 75\% goal
  for clinical systems {[}3{]}.
\item
  \textbf{AUROC Performance}: We measured AUROC of 0.9019.
\end{enumerate}

\hypertarget{data-integrity}{%
\subsection{Data Integrity}\label{data-integrity}}

All evaluations used: - 865 files from TUSZ v2.0.3 eval set (127.7
hours) - No data leakage (completely held-out test set) - Identical
post-processing across all scorers - merge\_gap disabled (no event
merging) for NEDC compliance

See Appendix Tables A1-A2 for full metrics; accompanying plots are
reproducible via \texttt{scripts/visualize\_results.py} and included in
the repository.

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\hypertarget{performance-gap-analysis}{%
\subsection{Performance Gap Analysis}\label{performance-gap-analysis}}

Our evaluation reveals a 27-137x gap between SeizureTransformer's
reported performance and its clinical reality on TUSZ. The model's
\textasciitilde1 FA/24h achievement on Dianalund becomes 26.89 FA/24h
with NEDC OVERLAP and 136.73 FA/24h with NEDC TAES when evaluated on its
training dataset. This dramatic variation is not an indictment of
SeizureTransformer's architecture, which represents a genuine advance in
combining U-Net feature extraction with Transformer sequence modeling.
Rather, it exposes fundamental issues in how the field evaluates seizure
detection models, where the same predictions can yield vastly different
performance metrics depending on evaluation choices.

\hypertarget{impact-of-scoring-methodology}{%
\subsection{Impact of Scoring
Methodology}\label{impact-of-scoring-methodology}}

The 3.1x difference in false alarm rates between NEDC OVERLAP (26.89
FA/24h) and SzCORE Event (8.59 FA/24h) on identical predictions
demonstrates that scoring methodology alone can determine whether a
model appears clinically viable. NEDC TAES, with its strict time-aligned
evaluation, shows an even larger 5.1x increase over OVERLAP and a 15.9x
increase over SzCORE Event. These differences stem from fundamental
philosophical disagreements about what constitutes a correct detection:
TAES requires precise temporal alignment and penalizes both over- and
under-segmentation through partial credit scoring, OVERLAP accepts any
temporal intersection as sufficient, while SzCORE Event adds 30-second
pre-ictal and 60-second post-ictal tolerances before applying overlap
logic. Each approach serves legitimate clinical purposes---TAES for
applications requiring precise seizure boundaries, OVERLAP for standard
clinical review, and SzCORE Event for screening where missing events is
costlier than false alarms.

Our focus on event-based metrics reflects clinical priorities. While
sample-based (epoch) methods (NEDC EPOCH; SzCORE Sample-based) can yield
high nominal accuracy by correctly classifying long non-seizure periods,
they obscure the core task of detecting seizure events. We therefore
restrict comparisons and conclusions to event-based scores.

\hypertarget{clinical-deployment-constraints}{%
\subsection{Clinical Deployment
Constraints}\label{clinical-deployment-constraints}}

The inability to achieve clinical viability reveals a critical gap
between research achievements and deployment readiness. Our best
operating point at 10 FA/24h achieved only 33.90\% sensitivity with NEDC
OVERLAP, falling far short of the 75\% sensitivity goal for clinical
systems {[}3{]}. This constraint is not merely academic---it determines
whether AI assistants can be deployed in ICUs, where false alarms cause
alarm fatigue and missed seizures delay critical treatment. While human
reviewers achieve approximately 1 FA/24h {[}3{]}, even at a more
permissive 10 FA/24h threshold, current models cannot approach the
sensitivity levels required for clinical deployment when evaluated with
appropriate standards.

\hypertarget{root-causes-of-evaluation-gaps}{%
\subsection{Root Causes of Evaluation
Gaps}\label{root-causes-of-evaluation-gaps}}

The performance disparities stem from multiple compounding factors
beyond scoring methodology. Dataset characteristics play a crucial role:
TUSZ contains 865 evaluation files with diverse seizure types and
recording conditions from an urban academic medical center, while
Dianalund represents a specialized epilepsy monitoring unit with
potentially cleaner recordings and different patient populations.
Training choices further compound these differences---SeizureTransformer
was trained on TUSZ v1.5.2 combined with the Siena dataset {[}7{]},
potentially creating distribution shifts even within TUSZ versions. The
lack of standardized evaluation protocols allows models to be tested on
favorable datasets with permissive scoring, creating an illusion of
clinical readiness that disappears under rigorous evaluation.

\hypertarget{systemic-issues-in-the-field}{%
\subsection{Systemic Issues in the
Field}\label{systemic-issues-in-the-field}}

The 27-137x gap we document is not unique to SeizureTransformer but
reflects systemic issues in how seizure detection research approaches
evaluation. The field has optimized for benchmark leaderboards rather
than clinical deployment, creating incentives to report results on
datasets and with scoring methods that maximize apparent performance.
EpilepsyBench's use of a train icon to mark TUSZ and withhold TUSZ
evaluation metrics, while well-intentioned to ensure held-out testing,
can inadvertently discourage evaluating models on TUSZ's held-out split
with matched tooling. This creates a situation where models can claim
state-of-the-art performance without ever facing the clinical standards
they purport to meet.

\hypertarget{cross-dataset-validity}{%
\subsection{Cross-Dataset Validity}\label{cross-dataset-validity}}

Using identical SzCORE Event scoring, SeizureTransformer achieves 1
FA/24h on Dianalund (37\% sensitivity) versus 8.59 FA/24h on TUSZ
(52.35\% sensitivity)---an 8.6x degradation that indicates limited
generalization across datasets even under permissive clinical
tolerances. This isolates dataset shift from scoring effects. When we
further apply TUSZ's standard NEDC OVERLAP scoring, the gap widens to
26.89 FA/24h (27x increase), and with strict NEDC TAES scoring reaches
136.73 FA/24h (137x increase). These cascading gaps---8.6x from dataset
alone, then 3.1x from scoring methodology, then another 5.1x from
temporal precision requirements---demonstrate how evaluation choices
compound to create order-of-magnitude performance variations.

\hypertarget{recommendations-for-transparent-evaluation}{%
\subsection{Recommendations for Transparent
Evaluation}\label{recommendations-for-transparent-evaluation}}

Addressing these challenges requires fundamental changes in evaluation
practices. First, models should always be evaluated on held-out portions
of their training datasets using dataset-matched scoring tools---TUSZ
with NEDC, CHB-MIT with their protocols, and private datasets with their
clinical standards. Second, papers must report performance across
multiple scoring methodologies, acknowledging that different clinical
applications require different evaluation approaches. Third, researchers
should provide complete operating point curves showing the full
sensitivity-false alarm tradeoff space, allowing clinicians to select
thresholds appropriate for their use cases. Finally, the community needs
to establish minimum reporting standards that include dataset version,
evaluation tool version, and complete post-processing parameters to
ensure reproducibility.

\hypertarget{limitations-and-scope}{%
\subsection{Limitations and Scope}\label{limitations-and-scope}}

Our evaluation focuses on a single model and dataset combination,
limiting generalizability to other architectures or datasets. We used
the authors' pretrained weights without retraining, preventing us from
exploring whether architectural modifications or training strategies
could close the performance gap. Our analysis is restricted to seizure
detection metrics without considering computational requirements,
latency, or other practical deployment constraints. Additionally, TUSZ
represents only one clinical context---academic medical center EEG---and
performance may differ in community hospitals, ICUs, or ambulatory
monitoring scenarios. These limitations emphasize the need for
comprehensive evaluation across multiple models, datasets, and clinical
contexts.

\hypertarget{future-directions}{%
\subsection{Future Directions}\label{future-directions}}

This work highlights several critical areas for future research. The
field urgently needs standardized evaluation protocols that specify
dataset versions, scoring tools, and reporting requirements. Models
should be developed with explicit clinical requirements as optimization
targets rather than benchmark metrics that may not reflect deployment
needs. Real-world validation studies comparing model predictions to
clinical outcomes would provide the ultimate test of utility beyond
detection metrics. The community should also explore whether ensemble
methods, domain adaptation, or clinical fine-tuning can bridge the gap
between benchmark and clinical performance. Most importantly, closer
collaboration between AI researchers and clinical practitioners is
essential to ensure that technical advances translate into patient
benefit rather than merely impressive benchmark scores.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

Our evaluation of SeizureTransformer on TUSZ's held-out test set reveals
a 27-137x gap between benchmark claims and clinical reality, with the
model producing 26.89 false alarms per 24 hours using NEDC OVERLAP
versus the \textasciitilde1 FA/24h achieved on Dianalund {[}8{]}. This
discrepancy stems not from model failure but from fundamental mismatches
in evaluation methodology. The same predictions yield 8.59 FA/24h with
SzCORE Event's permissive tolerances {[}6{]}, 26.89 FA/24h with NEDC
OVERLAP {[}2{]}, and 136.73 FA/24h with NEDC TAES {[}5{]}---a nearly
16-fold spread determined entirely by scoring philosophy. When optimized
for a 10 FA/24h threshold with NEDC scoring, the model achieves only
33.90\% sensitivity, falling far short of the 75\% sensitivity goal for
clinical systems {[}3{]}. These findings demonstrate that meaningful
progress in automated seizure detection requires evaluation standards
that match clinical reality rather than optimize benchmark metrics.

The path forward demands fundamental changes in how the field approaches
evaluation. Models must be evaluated on held-out portions of their
training datasets using dataset-matched scoring tools---TUSZ with NEDC,
CHB-MIT with their protocols, and private datasets with their clinical
standards. Papers should report performance across multiple scoring
methodologies, acknowledging that different clinical applications
require different evaluation approaches while maintaining transparency
about which methods are used. Complete operating curves showing the
sensitivity-false alarm tradeoff space enable clinicians to select
thresholds appropriate for their specific use cases. Most critically,
the community must establish minimum reporting standards that include
dataset version, evaluation tool version, and complete post-processing
parameters to ensure reproducibility. As seizure detection models
approach deployment readiness, the field stands at a crossroads:
continue optimizing for benchmarks that may mislead, or establish
rigorous evaluation standards that bridge the gap between laboratory
success and patient benefit. The 27-137x gap we document is not
insurmountable but requires the collective will to prioritize clinical
validity over benchmark performance.

\hypertarget{reproducibility-and-resources}{%
\section{Reproducibility and
Resources}\label{reproducibility-and-resources}}

\hypertarget{code-and-data-availability}{%
\subsection{Code and Data
Availability}\label{code-and-data-availability}}

\textbf{Evaluation Pipeline}:
\url{https://github.com/Clarity-Digital-Twin/SeizureTransformer}
\textbf{Release}: \texttt{v1.0-arxiv} \textbf{Model Weights}: Authors'
pretrained \texttt{model.pth} (168MB) from
\url{https://github.com/keruiwu/SeizureTransformer} \textbf{TUSZ
Dataset}: v2.0.3 via Data Use Agreement from
\url{https://isip.piconepress.com/projects/tuh_eeg/} \textbf{NEDC
Scorer}: v6.0.0 from \url{https://isip.piconepress.com/projects/nedc/}
(August 2025 release)

\hypertarget{computational-requirements}{%
\subsection{Computational
Requirements}\label{computational-requirements}}

\begin{itemize}
\tightlist
\item
  \textbf{Hardware}: NVIDIA GPU with \textgreater=8GB VRAM (RTX 3060 or
  better)
\item
  \textbf{Processing Time}: \textasciitilde8 hours for 865 TUSZ eval
  files on RTX 4090
\item
  \textbf{Storage}: 45GB for TUSZ eval set, 5GB for intermediate outputs
\item
  \textbf{Memory}: 16GB system RAM minimum
\end{itemize}

\hypertarget{exact-reproduction-procedure}{%
\subsection{Exact Reproduction
Procedure}\label{exact-reproduction-procedure}}

\hypertarget{environment-setup}{%
\subsubsection{1. Environment Setup}\label{environment-setup}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ clone https://github.com/Clarity{-}Digital{-}Twin/SeizureTransformer}
\BuiltInTok{cd}\NormalTok{ SeizureTransformer}
\ExtensionTok{uv}\NormalTok{ venv }\KeywordTok{\&\&} \BuiltInTok{source}\NormalTok{ .venv/bin/activate}
\ExtensionTok{uv}\NormalTok{ pip install {-}e . {-}{-}extra dev}
\end{Highlighting}
\end{Shaded}

\hypertarget{generate-model-predictions}{%
\subsubsection{2. Generate Model
Predictions}\label{generate-model-predictions}}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{tusz{-}eval} \KeywordTok{\textbackslash{}}
  \ExtensionTok{{-}{-}data\_dir}\NormalTok{ /path/to/tusz\_v2.0.3/edf/eval }\KeywordTok{\textbackslash{}}
  \ExtensionTok{{-}{-}out\_dir}\NormalTok{ experiments/eval/repro }\KeywordTok{\textbackslash{}}
  \ExtensionTok{{-}{-}device}\NormalTok{ cuda}
\end{Highlighting}
\end{Shaded}

\hypertarget{apply-nedc-clinical-scoring}{%
\subsubsection{3. Apply NEDC Clinical
Scoring}\label{apply-nedc-clinical-scoring}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Paper default (theta=0.8, k=5, d=2.0s)}
\ExtensionTok{nedc{-}run} \KeywordTok{\textbackslash{}}
  \ExtensionTok{{-}{-}checkpoint}\NormalTok{ experiments/eval/repro/checkpoint.pkl }\KeywordTok{\textbackslash{}}
  \ExtensionTok{{-}{-}outdir}\NormalTok{ results/nedc\_default }\KeywordTok{\textbackslash{}}
  \ExtensionTok{{-}{-}backend}\NormalTok{ nedc{-}binary }\KeywordTok{\textbackslash{}}
  \ExtensionTok{{-}{-}threshold}\NormalTok{ 0.80 {-}{-}kernel 5 {-}{-}min\_duration\_sec 2.0}

\CommentTok{\# Clinical 10 FA/24h target}
\ExtensionTok{nedc{-}run} \KeywordTok{\textbackslash{}}
  \ExtensionTok{{-}{-}checkpoint}\NormalTok{ experiments/eval/repro/checkpoint.pkl }\KeywordTok{\textbackslash{}}
  \ExtensionTok{{-}{-}outdir}\NormalTok{ results/nedc\_10fa }\KeywordTok{\textbackslash{}}
  \ExtensionTok{{-}{-}backend}\NormalTok{ nedc{-}binary }\KeywordTok{\textbackslash{}}
  \ExtensionTok{{-}{-}threshold}\NormalTok{ 0.88 {-}{-}kernel 5 {-}{-}min\_duration\_sec 3.0}
\end{Highlighting}
\end{Shaded}

Note: \texttt{nedc-run} is a thin wrapper intended for development use
from a repo checkout (or editable install). It expects the vendored NEDC
tree under \texttt{evaluation/nedc\_eeg\_eval/v6.0.0} to be available.
If running outside this repo context, invoke the vendored Makefile
directly:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make}\NormalTok{ {-}C evaluation/nedc\_eeg\_eval/nedc\_scoring }\KeywordTok{\textbackslash{}}
  \ExtensionTok{all}\NormalTok{ CHECKPOINT=../../experiments/eval/repro/checkpoint.pkl }\KeywordTok{\textbackslash{}}
  \VariableTok{OUTDIR=}\NormalTok{../../results/nedc\_default}
\end{Highlighting}
\end{Shaded}

\hypertarget{apply-szcore-event-comparison}{%
\subsubsection{4. Apply SzCORE Event
Comparison}\label{apply-szcore-event-comparison}}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{szcore{-}run} \KeywordTok{\textbackslash{}}
  \ExtensionTok{{-}{-}checkpoint}\NormalTok{ experiments/eval/repro/checkpoint.pkl }\KeywordTok{\textbackslash{}}
  \ExtensionTok{{-}{-}outdir}\NormalTok{ results/szcore\_default }\KeywordTok{\textbackslash{}}
  \ExtensionTok{{-}{-}threshold}\NormalTok{ 0.80 {-}{-}kernel 5 {-}{-}min\_duration\_sec 2.0}
\end{Highlighting}
\end{Shaded}

\hypertarget{generate-figures-and-tables}{%
\subsubsection{5. Generate Figures and
Tables}\label{generate-figures-and-tables}}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{python}\NormalTok{ scripts/visualize\_results.py {-}{-}results\_dir results/}
\CommentTok{\# Table compilation is integrated in evaluation scripts; see docs/results/* for generated summaries.}
\end{Highlighting}
\end{Shaded}

\hypertarget{key-implementation-details}{%
\subsection{Key Implementation
Details}\label{key-implementation-details}}

\begin{itemize}
\tightlist
\item
  \textbf{EDF Processing}: 19-channel unipolar montage, resampled to 256
  Hz
\item
  \textbf{Window Size}: 60-second non-overlapping windows (15,360
  samples)
\item
  \textbf{Post-processing}: Morphological operations with configurable
  kernel size
\item
  \textbf{CSV Format}: NEDC requires \texttt{.csv\_bi} extension with
  4-decimal precision
\item
  \textbf{Scoring Backends}: NEDC v6.0.0 (vendored, unmodified)
  orchestrated via a thin \texttt{nedc-run} CLI; predictions are
  converted to NEDC CSV\_bi and scored with the official binaries
\end{itemize}

\hypertarget{validation-checksums}{%
\subsection{Validation Checksums}\label{validation-checksums}}

To verify correct reproduction, key outputs should match: -
\texttt{checkpoint.pkl}: MD5 \texttt{3f8a2b...} (469 seizures detected)
- NEDC OVERLAP @ default: 26.89 +- 0.01 FA/24h - SzCORE Event @ default:
8.59 +- 0.01 FA/24h

\hypertarget{acknowledgments}{%
\section{Acknowledgments}\label{acknowledgments}}

We thank Joseph Picone and the Neural Engineering Data Consortium at
Temple University for creating and maintaining the TUSZ dataset and NEDC
evaluation tools, which enabled this rigorous assessment. We are
grateful to Kerui Wu and colleagues for making their SeizureTransformer
model weights publicly available, demonstrating exemplary commitment to
reproducible research. We acknowledge the EpilepsyBench initiative for
advancing standardized benchmarking in seizure detection, even as our
work highlights areas for improvement. Special thanks to the clinical
EEG experts whose annotations in TUSZ made dataset-matched evaluation
possible. This work used computational resources provided by the
authors' institution. The authors declare no competing interests.

\hypertarget{references}{%
\section{References}\label{references}}

{[}1{]} Wu K, Zhao Z, Yener B. SeizureTransformer: Scaling U-Net with
Transformer for Simultaneous Time-Step Level Seizure Detection from Long
EEG Recordings. International Conference on Artificial Intelligence in
Epilepsy and Other Neurological Disorders. 2025. arXiv:2504.00336.
{[}2{]} NEDC. Neural Engineering Data Consortium EEG Evaluation Software
v6.0.0. Temple University; 2025. {[}3{]} Beniczky S, Ryvlin P. Standards
for testing and clinical validation of seizure detection devices.
Epilepsia. 2018;59(S1):9-13. doi:10.1111/epi.14049. {[}4{]} Shah V, von
Weltin E, Lopez S, McHugh JR, Veloso L, Golmohammadi M, Obeid I, Picone
J. The Temple University Hospital Seizure Detection Corpus. Front
Neuroinform. 2018;12:83. doi:10.3389/fninf.2018.00083. {[}5{]} Shah V,
Golmohammadi M, Obeid I, Picone J. Objective Evaluation Metrics for
Automatic Classification of EEG Events. In: Signal Processing in
Medicine and Biology. Springer; 2021. p.~235-282. {[}6{]} Dan J, Pale U,
Amirshahi A, Cappelletti W, Ingolfsson TM, Wang X, et al.~SzCORE: A
Seizure Community Open-source Research Evaluation framework for the
validation of EEG-based automated seizure detection algorithms. 2024.
{[}7{]} Detti P. Siena Scalp EEG Database (version 1.0.0). PhysioNet.
2020. RRID:SCR\_007345. doi:10.13026/5d4a-j060. {[}8{]} EpilepsyBench
Consortium. EpilepsyBench: Seizure Detection Challenge and Benchmarks.
2025. Available from: https://epilepsybenchmarks.com {[}9{]} Haibe-Kains
B, Adam GA, Hosny A, Khodakarami F, Waldron L, Wang B, et
al.~Transparency and reproducibility in artificial intelligence. Nature.
2020;586(7829):E14-E16. {[}10{]} Gemein LAW, Schirrmeister RT,
Chrabąszcz P, Wilson D, Boedecker J, Schulze-Bonhage A, et
al.~Machine-learning-based diagnostics of EEG pathology. NeuroImage.
2020;220:117021. {[}11{]} Holger, Kern S, Papadopoulos Orfanos D, Vallat
R, Brunner C, Cerina L, Appelhoff S, et al.~pyEDFlib: v0.1.42. Zenodo;
2025. doi:10.5281/zenodo.15748763. {[}12{]} Roy S, Kiral I, Mirmomeni M,
et al.~Evaluation of artificial intelligence systems for assisting
neurologists with fast and accurate annotations of scalp
electroencephalography data. eBioMedicine. 2021;66:103275.
doi:10.1016/j.ebiom.2021.103275
