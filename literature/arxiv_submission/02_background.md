# Background and Related Work

The Temple University Hospital Seizure Corpus (TUSZ) represents the largest publicly available seizure dataset, containing 3,050 hours of continuous EEG from 642 unique patients with 3,964 expert-annotated seizure events (Shah et al., 2018). Critically, TUSZ implements patient-disjoint train/dev/eval splitsâ€”no patient appears in multiple splitsâ€”preventing data leakage and enabling valid generalization assessment. The evaluation set contains 865 EDF files totaling 127.7 hours from 43 patients with 469 seizures, specifically reserved for final held-out testing. This careful split design, with 486 patients in training, 53 in development, and 43 in evaluation, follows machine learning best practices often violated in medical AI applications. The annotations, performed by board-certified neurologists at Temple University Hospital, follow conservative clinical guidelines requiring clear electrographic seizures with definite evolution and temporal precision in marking onset and offset.

Alongside TUSZ, Temple University developed the Neural Event Detection Competition (NEDC) scoring software suite, creating a matched evaluation ecosystem. NEDC v6.0.0 provides the definitive scoring implementation for TUSZ evaluation. This matched pairing is no coincidenceâ€”the same research group created both the dataset and its evaluation tools, ensuring consistency between annotation philosophy and scoring methodology. NEDC implements multiple scoring modes, with OVERLAP (any-overlap binary scoring) serving as the clinical standard for TUSZ evaluation. The software is widely used in the literature and serves as a reference implementation for clinical seizure detection evaluation.

The choice of scoring methodology profoundly impacts reported performance, as different methods serve distinct clinical and research priorities. Time-Aligned Event Scoring (TAES), proposed by Picone et al. (2021), represents the strictest evaluation standard, computing partial credit based on temporal overlap percentageâ€”a 60-second seizure with 45 seconds correctly detected receives 0.75 true positive credit. TAES emphasizes temporal precision, making it ideal for algorithm development and research applications where exact timing matters. In contrast, OVERLAP scoring, which NEDC implements as its standard mode, treats any temporal overlap between prediction and ground truth as a full true positive. Picone et al. (2021) note that "OVLP is considered a very permissive way of scoring since any amount of overlap between a reference and hypothesis event constitutes a true positive," yet this binary approach has become the de facto standard for TUSZ evaluation, balancing clinical relevance with research needs.

At the most permissive end of the spectrum, SzCORE (Ebenezer et al., 2024) extends any-overlap scoring with clinical tolerances designed for real-world deployment. The system adds 30-second pre-ictal and 60-second post-ictal windows around each ground truth event, recognizing that early warnings before seizure onset provide clinical value and that EEG patterns normalize gradually after seizure termination. Additionally, SzCORE merges predictions separated by less than 90 seconds into single events, substantially reducing alarm fatigue in clinical settings. These modifications, while clinically motivated, can reduce reported false alarm rates by factors of 3-10Ã— compared to stricter scoring methods. Importantly, these different approaches represent not right or wrong methods but rather different valid perspectives on what constitutes meaningful seizure detectionâ€”research precision versus clinical utility versus deployment practicality.

SeizureTransformer (Wu et al., 2025) exemplifies both the advances and evaluation gaps in modern seizure detection. The architecture combines U-Net's biomedical segmentation capabilities with Transformer self-attention to capture local and global EEG patterns. Trained on a subset of TUSZ v1.5.2 (~910 hours) plus the Siena Scalp EEG Database (128 hours), the model processes 19â€‘channel EEG at 256 Hz through 60â€‘second windows. With roughly 41 million parameters and publicly available pretrained weights (â‰ˆ168 MB), SeizureTransformer won the EpilepsyBench Challenge, achieving 37% sensitivity at 1 false alarm per 24 hours on the Dianalund datasetâ€”a Danish longâ€‘term monitoring corpus distinct from its training data. The authors' decision to openly share their weights enables reproducible evaluation, a practice we build on here.

Despite training on TUSZ, SeizureTransformer has never been evaluated on TUSZ's held-out evaluation set using Temple's clinical scoring standard. EpilepsyBench marks TUSZ results with a train emoji (ðŸš‚), indicating the model was trained on this dataset and therefore showing no evaluation metrics. While this conservative approach prevents overfitting claims, it overlooks the careful patient-disjoint split design that specifically enables valid held-out evaluation. This represents a broader pattern in the field: models are trained on Dataset X, evaluated on Dataset Y with favorable scoring, generalization is claimed, yet performance on X's properly designed evaluation set remains unknown. The uniform application of SzCORE scoring across all EpilepsyBench datasets, while ensuring consistency, obscures dataset-specific performance that would be revealed by matched evaluation tools.

The clinical deployment of seizure detection systems requires meeting stringent performance thresholds, typically less than 10 false alarms per 24 hours with at least 50% sensitivity (Beniczky et al., 2024). These requirements reflect the reality of clinical workflows where excessive false alarms lead to alarm fatigue and system abandonment. However, whether a system meets these thresholds depends critically on the evaluation methodology employed. Previous work has highlighted challenges in cross-dataset generalization (Gemein et al., 2020), the need for standardized evaluation metrics (Ward et al., 2019), and broader reproducibility issues in medical AI (Haibe-Kains et al., 2020). Our work addresses these challenges by performing the missing evaluation: testing SeizureTransformer on TUSZ's held-out set using multiple scoring methodologies, revealing how evaluation choices fundamentally shape performance claims in seizure detection systems.
