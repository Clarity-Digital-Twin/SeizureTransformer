# Request for Clarity on TUH/TUSZ Reporting Policy

Date: 2025-09-12

To: Epilepsy Bench maintainers and related stakeholders

## Context

- Our team reproduced SeizureTransformer evaluation on TUH/TUSZ using the official NEDC EEG Eval v6.0.0 tools and the standard TUH splits (train/dev/eval).
- We noticed Epilepsy Bench suppresses TUH metrics (shown as "ğŸš‚") when a model was trained on TUH at all, even if results are reported strictly on the heldâ€‘out TUH eval split.

## Why Weâ€™re Writing

In standard ML practice, reporting inâ€‘distribution results on a heldâ€‘out test split (train â†’ dev â†’ eval) is valid and expected. Separately, crossâ€‘dataset generalization (e.g., training offâ€‘TUH and testing on TUH) is also valuable, but it serves a different goal.

The current Bench behavior appears to hide TUH results whenever TUH is used for training, which may conflate these two distinct evaluation regimes.

## Request

We respectfully ask for clarification and/or a policy update that distinguishes two tracks:

1) Inâ€‘Distribution (ID) â€” TUHâ€‘train for training, TUHâ€‘dev for tuning, TUHâ€‘eval for final reporting (strict holdâ€‘out, NEDC TAES).

2) Crossâ€‘Dataset (OOD) â€” Train on nonâ€‘TUH data (e.g., Siena/SeizeIT/Dianalund), evaluate on TUHâ€‘eval to measure outâ€‘ofâ€‘domain robustness.

Publishing both tracks sideâ€‘byâ€‘side would:

- Preserve the value of the TUH official split (ID generalization).
- Promote robustness comparisons across institutions (OOD generalization).
- Avoid penalizing valid, splitâ€‘respecting TUH eval reporting.

## Specific Questions

- Does the Bench currently hide TUH metrics for any submission that used TUH data in training, regardless of split provenance? If so, could that be reconsidered under an explicit â€œInâ€‘Distribution TUHâ€‘evalâ€ track?
- What evidence or artifacts would you require to display TUHâ€‘eval metrics under an ID track (e.g., training logs, data provenance, NEDC output summaries)?
- Are there recommended postâ€‘processing or threshold calibration procedures to align reported TUH results with Bench expectations (e.g., tuning thresholds on TUHâ€‘dev, reporting full TAES curves)?

## Our Method Summary (for reproducibility)

- Official scorer: NEDC EEG Eval v6.0.0 (inâ€‘repo), invoked via `$NEDC_NFC/bin/nedc_eeg_eval`.
- Inputs: hypothesis/reference `.csv_bi` with headers `version`, `bname`, `duration`; channel `TERM`; times in seconds with 4â€‘decimal precision.
- Lists: absolute paths generated for `ref.list` and `hyp.list`.
- Postâ€‘processing: threshold 0.8, morphological opening/closing (kernel=5 samples), minimum event duration 2s (mirrors OSS defaults).
- Reporting: we can provide full `summary_taes.txt`/`summary.txt` outputs for review.

## Proposed Outcome

If the Benchâ€™s goal is to emphasize crossâ€‘dataset robustness, we fully support an OOD track. We simply request that valid, splitâ€‘respecting TUHâ€‘eval results also be visible under an ID track. This separation aligns with standard ML practice and improves clarity for the community.

We appreciate your work on benchmarking and would be happy to share our evaluation scripts or collaborate on a minimal reproducible pipeline specification for TUH/TUSZ.

Sincerely,

Clarity Digital Twin â€” SeizureTransformer Team

